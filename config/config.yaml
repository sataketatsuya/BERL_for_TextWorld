general:
  discount_gamma: 0.9               # Discount factor for the return computation.
  hcp: 0                            # used handicap

tensorboard:
  directory: ''                     # Logging directory for tensorboard. If '' not logged.
  log_frequency: 240                # in seconds

checkpoint:
  experiment_tag: 'log'                                  # Name of the experiment.
  model_checkpoint_path: 'saved_models'                  # Path where the model should be saved.
  pretrained_experiment_path: 'weights/agent_weights'    # Path of ptetrained model. If path == '' nothing is loaded
  save_frequency: 3600                                   # in seconds

training:
  batch_size: 1                     # Batch size of the training.
  nb_epochs: 10                     # Number of epochs of the training.
  max_nb_steps_per_episode: 100     # After this many steps a game is terminated.
  update_frequency: 20              # After this many steps we perform the unrolling for the update.
  gamma: 0.99                       # Gamma for culculating discounted reward
  gae_lambda: 0.95                  # Gamma for culculating discounted reward
  policy_clip: 0.2                  # Policy Clip for clipped surrogate objective on PPO
  custom_template: True             # Use custom command template for Ner Bert Command Generator
  optimizer:
    alpha: 0.0005                   # Learning rate of the (Adam) optimizer.
    input_dims: 512                 # dimention model for Transformer and actor-critic input dimention

model:
  max_seq_length: 1024               # Size of the input sequence length for Transformer
  bert_model: 'bert-base-uncased'    # Bert Model Pretrained Name
  batch_size: 1                      # Batch size of the model
  hidden_size: 256                   # Size of the hidden dimension for the GRU encoders.
  hidden_linear_size: 256            # Size of the hidden dimenison for the FC models in the scorers.
  epsilon: 0.1                       # epsilon greedy parameter
